# Refactoring Plan for integrating the `llm` crate into "Vespe"

**Goal:** Replace custom LLM interaction logic with the `llm` crate to leverage its unified API, prompt templating, and other features, thereby simplifying the codebase and improving maintainability.

## Phase 1: Initial Integration and Setup

1.  **Verify `llm` crate version:** The `llm` crate is already added to `Cargo.toml` with version `1.3.4`. No action needed here.
2.  **Basic `llm` crate usage (Proof of Concept):**
    *   In a new temporary file (e.g., `src/llm/temp_llm_poc.rs`), write a minimal example using the `llm` crate to perform a simple text completion or chat interaction with one of the supported providers (e.g., OpenAI or Ollama). This will serve as a proof of concept and help understand the `llm` crate's API. This file will be removed after successful integration.

## Phase 2: Refactoring `src/llm` Module

1.  **Remove redundant LLM client code:**
    *   Deprecate or remove `src/llm/llm_client.rs` as the `llm` crate will provide the core client functionality.
    *   Deprecate or remove `src/llm/providers/ollama.rs` and `src/llm/providers/openai.rs`. The `llm` crate will handle the specifics of interacting with these providers.
    *   Update `src/llm/mod.rs` to reflect these changes.
2.  **Adapt `src/llm/models.rs`:** Review `src/llm/models.rs` and adapt its structures (`ChatMessage`, `LlmResponse`) to align with the `llm` crate's input/output models. If the `llm` crate provides comprehensive models, `src/llm/models.rs` might become significantly smaller or even be removed.
3.  **Integrate `llm` crate's client:**
    *   Introduce the `llm` crate's client (e.g., `llm::Client` or a similar entry point) into `src/llm/mod.rs` or a new `src/llm/llm_adapter.rs` file. This adapter will wrap the `llm` crate's client and expose a simplified interface compatible with `vespe`'s `LlmClient` trait (if we decide to keep a custom trait for flexibility).

## Phase 3: Refactoring `src/agent` Module

1.  **Replace `prompt_builder.rs`:**
    *   Deprecate or remove `src/agent/core/prompt_builder.rs`.
    *   Utilize the `llm` crate's prompt templating features for constructing prompts. This might involve adapting existing prompt templates (from `.vespe/prompts/system_prompt_template.txt`) to the `llm` crate's format.
2.  **Adapt `response_parser.rs`:**
    *   Review `src/agent/core/response_parser.rs`. The `llm` crate will return structured responses, so the parsing logic might need to be adapted or simplified to work with the `llm` crate's output, especially concerning `AgentAction` parsing.
3.  **Integrate Function Calling/Tools:**
    *   Investigate how the `llm` crate's "Function calling" feature can be integrated with "Vespe"'s `src/tools` and `src/agent/actions.rs`.
    *   Determine if "Vespe"'s `Tool` trait and `ToolRegistry` can be adapted to work with the `llm` crate's function calling mechanism, or if a new approach is needed. The goal is to map `vespe`'s `Tool` definitions to `llm` crate's function definitions.

## Phase 4: Testing and Verification

1.  **Update existing tests:** Modify any existing tests that interact with the LLM layer to use the new `llm` crate-based implementation.
2.  **Add new tests:** Write new tests to cover the integration of the `llm` crate and ensure all functionalities work as expected.
3.  **End-to-end testing:** Perform end-to-end tests of the "Vespe" application to ensure that agents can still interact with LLMs and tools correctly.

## Phase 5: Cleanup

1.  Remove all deprecated and unused code.
2.  Update documentation and comments to reflect the changes.