# Piano di Lavoro: Refactoring della Pipeline di Dati

Data: 2025-09-19

## 1. Problema

I test iniziali hanno rivelato che il modello addestrato produce un output completamente errato e insensato. L'analisi ha confermato l'ipotesi dell'utente: la causa è una discrepanza fondamentale nel processo di tokenizzazione tra la fase di training e la fase di inferenza.

-   **Training:** I dati venivano pre-processati usando una semplice divisione basata su spazi (`.split()`), e poi dati in pasto al tokenizer.
-   **Inferenza:** Il tokenizer riceveva la stringa di testo grezza, che gestisce la punteggiatura e i caratteri speciali in modo completamente diverso.

Questa discrepanza rende il modello addestrato inutilizzabile.

## 2. Soluzione Proposta: Pipeline basata su Spans

Per risolvere questo problema alla radice, è necessario refactorizzare la pipeline di preparazione dei dati per renderla robusta e garantire che la tokenizzazione sia identica in ogni fase. L'approccio corretto, standard per task di Token Classification/NER, è basato su "spans" a livello di carattere.

Il nuovo workflow sarà il seguente:

### Passo 1: Modifica di `prepare_dataset.py`

Lo script verrà modificato per produrre un formato `JSONL` più robusto. Invece di salvare parole pre-tokenizzate, salverà il testo completo e le posizioni (inizio/fine) di ogni segmento etichettato.

**Nuovo Formato di Output (`dataset.jsonl`):**
Ogni riga sarà un oggetto JSON con due chiavi:
1.  `full_text`: La stringa completa e non modificata dell'esempio.
2.  `spans`: Una lista di dizionari, dove ogni dizionario rappresenta un segmento etichettato.

**Esempio di riga in `dataset.jsonl`:**
```json
{
  "full_text": "THOUGHT: I need a tool. TOOL_CALL: do_stuff()",
  "spans": [
    {"label": "THOUGHT", "start": 9, "end": 24},
    {"label": "TOOL_CALL", "start": 37, "end": 48}
  ]
}
```

Questo formato disaccoppia la definizione dei dati dalla tokenizzazione.

### Passo 2: Modifica di `train.py`

La funzione di pre-processing (`tokenize_and_align_labels`) verrà riscritta per lavorare con il nuovo formato dati.

**Nuova Logica della Funzione:**
1.  Prende in input un esempio (`full_text` e `spans`).
2.  Tokenizza l'intero `full_text` in una sola passata. Il tokenizer, oltre agli `input_ids`, fornisce un `offset_mapping` che mappa ogni nuovo token alla sua posizione (inizio, fine) nella stringa originale.
3.  Inizializza una lista di etichette per i token, tutte impostate a "O" (Outside).
4.  Itera sulla lista degli `spans`:
    a. Per ogni span (es. `{"label": "THOUGHT", "start": 9, "end": 24}`), itera sui token.
    b. Usando l'`offset_mapping`, controlla quali token si trovano all'interno di quello span di caratteri.
    c. Assegna l'etichetta `B-THOUGHT` al primo token che inizia all'interno dello span e `I-THOUGHT` a tutti gli altri token che cadono (anche parzialmente) all'interno dello span.
5.  Questa logica di allineamento precisa garantisce che le etichette corrispondano perfettamente ai token generati, indipendentemente da come il tokenizer suddivide le parole o gestisce la punteggiatura.

### Passo 3: Modifica di `test_model.py`

Lo script di inferenza (`test_model.py`) è già quasi corretto, poiché parte dal testo grezzo. Le modifiche saranno minime e riguarderanno principalmente la funzione di post-processing per raggruppare correttamente i token etichettati.

## 3. Vantaggi di Questo Approccio

-   **Robustezza:** Il processo è immune a differenze di gestione di spazi, punteggiatura, ecc.
-   **Standard Industriale:** È l'approccio standard e raccomandato per l'addestramento di modelli NER/Token Classification.
-   **Garanzia di Consistenza:** La tokenizzazione è eseguita in un unico modo, sia in training che in inferenza, garantendo che il modello operi su dati coerenti.
