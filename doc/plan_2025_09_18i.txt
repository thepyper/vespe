# Piano di Implementazione: Binario di Test Ollama

**Obiettivo:** Creare un binario di test (`ollama_test_client`) per connettersi a un modello Ollama, inviare un prompt di sistema predefinito e una query utente, e stampare la risposta del modello. Questo servirà a testare l'interfaccia tool/model con prompt normativi.

---

## Fasi di Implementazione:

### Fase 1: Setup del Progetto e Dipendenze

1.  **Modifica `Cargo.toml`**:
    *   Aggiungere una nuova sezione `[[bin]]` per il binario `ollama_test_client`.
    *   Aggiungere le seguenti dipendenze nella sezione `[dependencies]`:
        *   `clap = { version = "4", features = ["derive"] }` per la gestione degli argomenti da riga di comando.
        *   `reqwest = { version = "0.12", features = ["json", "blocking"] }` per le richieste HTTP (useremo la versione `blocking` per semplicità in questo test).
        *   `tokio = { version = "1", features = ["full"] }` per l'esecuzione asincrona (anche se useremo `blocking` per `reqwest`, `tokio` è spesso una dipendenza implicita in progetti Rust moderni che usano `reqwest`).
        *   `serde = { version = "1", features = ["derive"] }` per la serializzazione/deserializzazione JSON.
        *   `serde_json = "1"` per lavorare con JSON.

2.  **Creazione del file sorgente**:
    *   Creare il file `src/bin/ollama_test_client.rs`.

### Fase 2: Parsing degli Argomenti da Riga di Comando

1.  **Definizione della struttura degli argomenti**:
    *   Utilizzare `clap` per definire gli argomenti:
        *   `--model <MODEL_NAME>`: Nome del modello Ollama (es. `llama3`). Obbligatorio.
        *   `--query <USER_QUERY>`: La query dell'utente da inviare al modello. Obbligatorio.
        *   `--ollama-url <URL>`: URL dell'istanza Ollama (es. `http://localhost:11434`). Opzionale, con un valore predefinito.
        *   `--system-prompt-path <PATH>`: Percorso al file del prompt di sistema (es. `doc/normative_mcp_prompt_1.txt`). Opzionale, con un valore predefinito.

### Fase 3: Lettura del Prompt di Sistema

1.  **Funzione per la lettura del file**:
    *   Implementare una funzione per leggere il contenuto del file specificato da `--system-prompt-path`.
    *   Gestire gli errori di lettura del file.

### Fase 4: Interazione con l'API Ollama

1.  **Strutture dati per la richiesta/risposta**:
    *   Definire le strutture `serde` per il payload della richiesta (messaggi, modello) e per la risposta dell'API Ollama.
    *   Esempio di struttura per la richiesta:
        ```rust
        #[derive(serde::Serialize)]
        struct OllamaRequest {
            model: String,
            messages: Vec<Message>,
            stream: bool, // false per ottenere la risposta completa in una volta
        }

        #[derive(serde::Serialize, serde::Deserialize)]
        struct Message {
            role: String,
            content: String,
        }
        ```
    *   Esempio di struttura per la risposta (semplificata):
        ```rust
        #[derive(serde::Deserialize)]
        struct OllamaResponse {
            message: Message,
            // Altri campi se necessari
        }
        ```

2.  **Costruzione della richiesta**:
    *   Creare un vettore di `Message` contenente il prompt di sistema e la query dell'utente.
    *   Costruire l'oggetto `OllamaRequest`.

3.  **Invio della richiesta HTTP**:
    *   Utilizzare `reqwest::blocking::Client` per inviare una richiesta POST all'endpoint `/api/chat` dell'URL Ollama.
    *   Impostare l'header `Content-Type` a `application/json`.
    *   Inviare il payload JSON.

4.  **Parsing e Stampa della Risposta**:
    *   Parsare la risposta JSON in un oggetto `OllamaResponse`.
    *   Stampare il contenuto del messaggio di risposta del modello sulla console.

### Fase 5: Gestione degli Errori

1.  Implementare una gestione robusta degli errori per:
    *   Parsing degli argomenti da riga di comando.
    *   Lettura del file del prompt di sistema.
    *   Errori di rete durante la richiesta HTTP.
    *   Errori di serializzazione/deserializzazione JSON.

### Fase 6: Logging delle Interazioni Raw

1.  **Nuovo argomento da riga di comando**:
    *   Aggiungere `--log-file <PATH>`: Percorso opzionale per un file di log dove salvare le interazioni raw (richiesta e risposta JSON).

2.  **Implementazione del logging**:
    *   Prima di inviare la richiesta HTTP, serializzare la `OllamaRequest` in una stringa JSON e stamparla a console e, se specificato, scriverla nel file di log.
    *   Dopo aver ricevuto la risposta HTTP, stampare la risposta raw (testo) a console e, se specificato, scriverla nel file di log.
    *   Aggiungere timestamp alle voci di log per facilitare il debug.

### Fase 7: Creazione Placeholder per il Prompt di Sistema

1.  Creare un file vuoto `doc/normative_mcp_prompt_1.txt` come placeholder.

---

## Esempio di Utilizzo (Target)

```bash
cargo run --bin ollama_test_client -- --model llama3 --query "Qual è la capitale della Francia?" --ollama-url http://localhost:11434 --system-prompt-path doc/normative_mcp_prompt_1.txt --log-file ollama_interactions.log
```

---

Questo piano copre tutti i passaggi necessari per implementare il binario di test richiesto.
