# Piano di Implementazione: Binario `ollama_agent_client`

**Obiettivo:** Creare un binario (`ollama_agent_client`) che estenda le funzionalità di `ollama_test_client` aggiungendo il parsing rigoroso delle risposte del modello secondo i template definiti in `doc/normative_mcp_prompt_1.txt` e un ciclo di esecuzione degli strumenti (`echo`, `read_file`, `write_file` finto).

---

## Fasi di Implementazione:

### Fase 1: Setup del Progetto e Dipendenze

1.  **Modifica `Cargo.toml`**:
    *   Aggiungere una nuova sezione `[[bin]]` per il binario `ollama_agent_client`.
    *   Le dipendenze saranno le stesse di `ollama_test_client` (`clap`, `reqwest`, `tokio`, `serde`, `serde_json`, `chrono`).

2.  **Creazione del file sorgente**:
    *   Creare il file `src/bin/ollama_agent_client.rs`.

### Fase 2: Parsing degli Argomenti da Riga di Comando

1.  **Definizione della struttura degli argomenti**:
    *   Utilizzare `clap` per definire gli stessi argomenti di `ollama_test_client`:
        *   `--model <MODEL_NAME>`: Nome del modello Ollama. Obbligatorio.
        *   `--query <USER_QUERY>`: La query dell'utente. Obbligatorio.
        *   `--ollama-url <URL>`: URL dell'istanza Ollama (default: `http://localhost:11434`).
        *   `--system-prompt-path <PATH>`: Percorso al file del prompt di sistema (default: `doc/normative_mcp_prompt_1.txt`).
        *   `--log-file <PATH>`: Percorso opzionale per il file di log.

### Fase 3: Lettura del Prompt di Sistema

1.  **Funzione per la lettura del file**:
    *   Leggere il contenuto del file specificato da `--system-prompt-path`.
    *   Gestire gli errori di lettura del file.

### Fase 4: Interazione con l'API Ollama (Richiesta Iniziale)

1.  **Strutture dati per la richiesta/risposta**:
    *   Mantenere le strutture `OllamaRequest` e `Message` come in `ollama_test_client`.
    *   Definire nuove strutture `serde` per il parsing delle risposte del modello secondo i tre template.

2.  **Costruzione e invio della richiesta**:
    *   Costruire e inviare la richiesta POST all'API Ollama, includendo il prompt di sistema e la query iniziale dell'utente.

3.  **Logging**:
    *   Registrare la richiesta raw inviata e la risposta raw ricevuta, sia a console che nel file di log (se specificato).

### Fase 5: Parsing delle Risposte e Ciclo di Azione

1.  **Parsing Rigoroso delle Risposte**:
    *   Implementare una logica di parsing che tenti di deserializzare la risposta JSON del modello in una delle tre strutture di template (`Tool Call`, `Regular Response`, `Thinking`).
    *   Utilizzare un `enum` per rappresentare i diversi tipi di risposta parsati.
    *   Gestire strettamente gli errori di parsing: se la risposta non corrisponde esattamente a uno dei template, segnalare un errore.

2.  **Ciclo di Azione**:
    *   Dopo aver parsato la risposta del modello:
        *   Se `Regular Response` o `Thinking`: Stampare il contenuto/pensieri e terminare l'esecuzione (o attendere una nuova query utente, ma per ora, esecuzione singola).
        *   Se `Tool Call`:
            *   Estrarre `TOOL_NAME` e `arguments`.
            *   Eseguire lo strumento corrispondente (vedi Fase 6).
            *   Il risultato dell'esecuzione dello strumento verrà formattato come un nuovo messaggio utente e inviato nuovamente all'LLM, continuando il ciclo.

### Fase 6: Implementazione degli Strumenti

1.  **`echo` tool**:
    *   Funzione: `fn execute_echo(text: String) -> String`.
    *   Implementazione: Restituisce il testo in input.

2.  **`read_file` tool**:
    *   Funzione: `fn execute_read_file(path: String, encoding: Option<String>) -> Result<String, String>`.
    *   Implementazione: Legge il contenuto del file dal `path` specificato. Gestisce l'`encoding` (default UTF-8). Restituisce `Ok(contenuto)` o `Err(messaggio_errore)`.

3.  **`write_file` (finto) tool**:
    *   Funzione: `fn execute_write_file(path: String, content: String, mode: String) -> String`.
    *   Implementazione: Stampa a console: "Faking write to `{path}` with content: `{content}` in mode: `{mode}`". Non scrive realmente su disco.

### Fase 7: Logging Esteso

1.  Estendere il logging per includere:
    *   Le risposte del modello parsate.
    *   Le chiamate agli strumenti e i loro output.
    *   I nuovi messaggi inviati all'LLM dopo l'esecuzione degli strumenti.

### Fase 8: Gestione degli Errori

1.  Gestione completa degli errori per tutti i nuovi componenti: parsing, esecuzione degli strumenti e il ciclo di azione.

---

## Esempio di Utilizzo (Target)

```bash
cargo run --bin ollama_agent_client -- --model llama3 --query "Leggi il file config.txt" --ollama-url http://localhost:11434 --system-prompt-path doc/normative_mcp_prompt_1.txt --log-file agent_interactions.log
```

---

Questo piano dettagliato copre tutte le funzionalità richieste per il binario `ollama_agent_client`.
