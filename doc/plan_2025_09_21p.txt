# Plan for Ollama API Streaming and Real-time Display

This plan addresses the user's request to implement streaming responses from the Ollama API for real-time display, while minimizing code complexity.

## Understanding Ollama Streaming

The Ollama API supports streaming responses, sending data in chunks as it's generated. This allows for real-time display of the LLM's output.

## Proposed Approach: Manual `reqwest` Streaming (Recommended for minimal disruption)

This approach leverages the existing `reqwest` crate to handle the streaming, requiring modifications primarily within the `ollama_client.rs` module.

### 1. Modify `src/bin/data_generator/ollama_client.rs`

-   **Change `query_ollama` function signature:** The function will need to be adapted to return a stream of `String` chunks (or similar) instead of a single `String`.
    -   **Example (conceptual):** `pub async fn query_ollama(...) -> Result<impl Stream<Item = Result<String>>>`
-   **Set `Accept` header:** Ensure the HTTP request includes an `Accept` header like `application/x-ndjson` to signal to Ollama that streaming is desired.
-   **Process `response.bytes_stream()`:** Instead of awaiting a full response, use `response.bytes_stream()` to get an asynchronous stream of byte chunks.
-   **Chunk Parsing:** Iterate over the incoming byte chunks. Ollama's streaming responses are typically newline-delimited JSON objects. Each chunk might contain a partial JSON object, a full one, or multiple. Robust parsing will be needed to:
    -   Accumulate bytes until a complete JSON object is formed.
    -   Parse the JSON object to extract the generated text (e.g., `response.content`).
    -   Handle potential errors during parsing or incomplete chunks.
-   **Yield Text Chunks:** Each extracted text chunk should be yielded from the function, allowing the caller to process it in real-time.

### 2. Update `src/bin/data_generator/pipeline.rs`

-   **Modify `get_student_response`:** This function currently calls `query_ollama`. It will need to be updated to handle the stream returned by the modified `query_ollama`.
    -   If the goal is to still return a single `(String, String)` (full response and system prompt), then `get_student_response` would need to consume the entire stream, concatenate the chunks, and then return the complete string. This would negate the real-time display benefit in `main.rs`.
    -   **Alternative:** If real-time display is desired in `main.rs`, `get_student_response` might also need to return a stream, or `main.rs` would directly call the streaming version of `query_ollama`.

### 3. Update `src/bin/data_generator/main.rs`

-   **Process the stream:** The loop in `main.rs` that calls `get_student_response` would need to iterate over the incoming text chunks from the stream and print them to the console (or display them in a UI) as they arrive.

## Alternative: Using the `llm` crate (version 1.3.4)

### What it is:

The `llm` crate is a Rust library for running local large language models directly within a Rust application. It's designed for direct model inference, not as a client for the Ollama API.

### How it would help:

-   **Direct Model Execution:** If the goal is to *replace* Ollama entirely and run models directly in Rust, the `llm` crate would be highly beneficial. It handles model loading, inference, and supports streaming generation internally.
-   **No External Server:** Removes the dependency on the Ollama server.

### Why it's a more complex alternative for *Ollama streaming*:

-   **Architectural Change:** Integrating the `llm` crate would be a significant architectural shift, as it bypasses the Ollama server. This is not just about adding streaming to the existing setup; it's about changing how the LLM is run.
-   **Model Management:** You would need to manage model weights directly within your Rust application (downloading, storing, loading), which Ollama currently handles.

## Conclusion

For easily implementing streaming with the *current Ollama setup*, modifying `ollama_client.rs` to handle `reqwest` streaming (Option 1) is the most direct path. The `llm` crate is a powerful alternative if you wish to move away from Ollama and run models directly in Rust, but it represents a more substantial change.

I recommend proceeding with **Option 1** if the primary goal is to add streaming to the existing Ollama integration without over-complicating the code. If you are open to a larger architectural change to remove the Ollama server dependency, then the `llm` crate would be the way to go.
