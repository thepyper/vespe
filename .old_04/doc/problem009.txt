c:\Proj\xxx\025_006.Vespe\vespe\buzz\training>python test_model.py --system-prompt ..\..\doc\normative_mcp_prompt_1.txt --prompt "read the file dio/cane and summarize it"

>>> Querying Ollama model 'gemma3:1b'...
--- Ollama Request Payload ---
[
  {
    "role": "system",
    "content": "..\\..\\doc\\normative_mcp_prompt_1.txt"
  },
  {
    "role": "user",
    "content": "read the file dio/cane and summarize it"
  }
]
----------------------------
<<< Ollama response:
Please provide me with the file `doc/cane`! I need to see the content of that file to summarize it for you. I don’t have access to local files on your computer.

Once you upload the file, I’ll be happy to help.

>>> Loading tokenizer and ONNX session...

>>> Running inference...
Input is shorter than window size, running simple inference.

>>> Parsed Segments:
  - [THOUGHT]: please provide me with the file
  - [TEXT]: `
  - [THOUGHT]: doc / cane ` ! i need to see the content of that file to sum mar ize it for you . i don ’ t have access to local files on your computer . once you up load the file , i ’ ll be happy to help .
