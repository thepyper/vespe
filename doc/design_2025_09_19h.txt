# Piano di Lavoro: Refactoring della Pipeline di Dati

Data: 2025-09-19

## 1. Problema

I test iniziali hanno rivelato che il modello addestrato produce un output completamente errato. La causa è una **discrepanza fondamentale nel processo di tokenizzazione** tra la fase di training e la fase di inferenza. Il training usava una tokenizzazione ingenua basata su spazi, mentre l'inferenza usava un tokenizer professionale che gestisce la punteggiatura e le "sub-word" in modo diverso, creando una totale confusione nel modello.

## 2. Soluzione: Pipeline basata su Spans di Caratteri

Per risolvere il problema alla radice, la pipeline di preparazione dei dati verrà refattorizzata secondo un approccio standard e robusto basato su "spans" (intervalli) di caratteri.

### Passo 1: Modifica di `prepare_dataset.py`

Lo script produrrà un file `JSONL` dove ogni riga contiene il testo completo dell'esempio e le coordinate (inizio/fine) di ogni segmento.

- **Nuovo Formato Esempio:**
```json
{
  "full_text": "THOUGHT: I need a tool. TOOL_CALL: do_stuff()",
  "spans": [
    {"label": "THOUGHT", "start": 9, "end": 24},
    {"label": "TOOL_CALL", "start": 37, "end": 48}
  ]
}
```

### Passo 2: Modifica di `train.py`

La funzione di pre-processing verrà riscritta per usare gli `spans` e l'`offset_mapping` del tokenizer per etichettare in modo preciso e inequivocabile i token "sub-word" generati dal testo grezzo.

## 3. Chiarimento sul Processo di Tokenizzazione

È importante chiarire due punti emersi dalla discussione:

### A. Non si tratta di Token per Carattere

Questo nuovo approccio **non** significa che passiamo a una tokenizzazione per carattere. Il modello e il tokenizer rimangono gli stessi (`DistilBERT`), che operano a livello di **sub-word**. Il cambiamento è nel modo in cui usiamo gli indici dei caratteri come "mappa" infallibile per assegnare le etichette corrette ai token sub-word, garantendo consistenza tra training e inferenza.

### B. Chi Tokenizza? Il Tokenizer, non il Modello

La tokenizzazione è gestita da un componente specializzato e separato: il **Tokenizer**. Il **Modello**, invece, è il componente che esegue i calcoli matematici sui numeri (ID dei token) prodotti dal tokenizer.

-   **Il Tokenizer:** È un "traduttore" che converte il testo in una sequenza di ID numerici che il modello può capire.
-   **Il Modello:** È un "matematico" che processa questi numeri per fare le predizioni.

Ogni modello ha il suo tokenizer specifico con cui è stato addestrato. È fondamentale usare sempre la coppia corrispondente (es. il modello `distilbert-base-uncased` con il tokenizer `distilbert-base-uncased`).
