# Piano per l'implementazione del "Model in the Middle"

## Obiettivo
Aggiungere una funzionalità opzionale a `ollama_agent_client.rs` che permetta di far processare l'output del modello principale da un secondo modello ("model in the middle") per una correzione sintattica, configurabile tramite opzioni CLI.

## Fasi

### Fase 0: Definizione degli argomenti CLI
1.  **Modificare `src/cli/commands.rs` (o dove sono definiti gli argomenti CLI per `ollama_agent_client`) per aggiungere i seguenti argomenti:**
    *   `--middle-model <MODEL_NAME>`: (Opzionale) Specifica il nome del modello da utilizzare per la correzione sintattica. Se non fornito, la funzionalità è disabilitata.
    *   `--middle-model-prompt-file <FILE_PATH>`: (Opzionale) Specifica il percorso assoluto al file contenente il prompt per il "model in the middle". Questo argomento sarà richiesto se `--middle-model` è specificato.

### Fase 1: Integrazione della logica del "Model in the Middle" in `ollama_agent_client.rs`
1.  **Parsare i nuovi argomenti CLI:** Recuperare i valori di `--middle-model` e `--middle-model-prompt-file`.
2.  **Condizione di attivazione:** Se `--middle-model` è stato fornito:
    *   **Caricamento del prompt:** Leggere il contenuto del file specificato da `--middle-model-prompt-file`. Gestire eventuali errori di lettura del file.
    *   **Preparazione della richiesta al modello intermedio:**
        *   Prendere l'output del modello principale.
        *   Combinare il prompt caricato dal file con l'output del modello principale per formare il nuovo input per il "model in the middle".
        *   Creare una nuova richiesta API utilizzando il modello specificato da `--middle-model` e l'input combinato.
    *   **Chiamata al modello intermedio:** Inviare la richiesta al "model in the middle" tramite l'esistente `LlmClient` o una sua istanza.
    *   **Utilizzo dell'output:** Sostituire l'output originale del modello principale con l'output ricevuto dal "model in the middle" come risultato finale.
3.  **Comportamento predefinito:** Se `--middle-model` non è fornito, il flusso di esecuzione deve rimanere invariato, utilizzando direttamente l'output del modello principale.

### Fase 2: Gestione degli errori e validazione
1.  **Validazione degli argomenti:**
    *   Assicurarsi che `--middle-model-prompt-file` sia presente se `--middle-model` è specificato.
    *   Verificare che il file specificato da `--middle-model-prompt-file` esista e sia leggibile.
2.  **Gestione errori API:** Implementare una robusta gestione degli errori per le chiamate al "model in the middle" (es. errori di rete, risposte non valide dal modello).

### Fase 3: Test
1.  **Test manuali:** Verificare il funzionamento con e senza l'opzione `--middle-model`, provando diversi modelli e file di prompt.
2.  **Test unitari/di integrazione (se applicabile):** Considerare l'aggiunta di test per la logica di integrazione del "model in the middle" e la gestione degli argomenti CLI.

## Considerazioni
*   Assicurarsi che l'interazione con l'LLM client sia riutilizzabile per il "model in the middle".
*   Definire chiaramente come il prompt del "model in the middle" deve essere combinato con l'output del primo modello (es. prepending, appending, wrapping). Per ora, assumeremo un semplice prepending del prompt all'output del primo modello.
*   La correzione sintattica implica che il "model in the middle" dovrebbe essere in grado di comprendere e riformattare l'output del primo modello.
